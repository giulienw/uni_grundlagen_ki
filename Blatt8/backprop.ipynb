{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Blatt 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e9b3c5",
      "metadata": {},
      "source": [
        "## NN.Backprop.01\n",
        "\n",
        "Notation\n",
        "\n",
        "- Aktivierung: $a^{(l)} = \\sigma(z^{(l)})$, $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$\n",
        "- Fehlerfunktion (Quadratfehler): $E = \\frac{1}{2} \\lVert a^{(3)} - y \\rVert^2$\n",
        "\n",
        "Rückwärts:\n",
        "\n",
        "- Ausgabe: $\\delta^{(3)} = (a^{(3)} - y) \\odot \\sigma'(z^{(3)})$\n",
        "- Versteckte Schicht 2: $\\delta^{(2)} = (W^{(3)T} \\delta^{(3)}) \\odot \\sigma'(z^{(2)})$\n",
        "- Versteckte Schicht 1: $\\delta^{(1)} = (W^{(2)T} \\delta^{(2)}) \\odot \\sigma'(z^{(1)})$\n",
        "\n",
        "Gewicht in der ersten versteckten Schicht (von Neuron $i$ in Schicht 0 nach $j$ in Schicht 1):\n",
        "\n",
        "- $\\frac{\\partial E}{\\partial w_{ij}^{(1)}} = a_i^{(0)} \\cdot \\delta_j^{(1)}$\n",
        "- Update: $w_{ij}^{(1)} \\leftarrow w_{ij}^{(1)} - \\alpha \\cdot a_i^{(0)} \\cdot \\delta_j^{(1)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebb6e6c",
      "metadata": {},
      "source": [
        "## NN.Backprop.02\n",
        "\n",
        "Gewichte aus der Abbildung:\n",
        "\n",
        "- $x \\to h$: $w_{xh} = -1$\n",
        "- bias $\\to h$: $w_{bh} = 1$\n",
        "- $h \\to y$: $w_{hy} = 1$\n",
        "- $x \\to y$: $w_{xy} = 2$\n",
        "- bias $\\to y$: $w_{by} = -2$\n",
        "\n",
        "Forward für $(x, y_T) = (0, 0.5)$:\n",
        "\n",
        "- $z_h = w_{bh} \\cdot 1 + w_{xh} \\cdot x = 1$\n",
        "- $h = \\sigma(z_h) = 0.7310585786$\n",
        "- $z_y = w_{by} \\cdot 1 + w_{hy} \\cdot h + w_{xy} \\cdot x = -1.2689414214$\n",
        "- $y = \\sigma(z_y) = 0.2194385171$\n",
        "\n",
        "Fehler:\n",
        "\n",
        "- $E = 0.5 \\cdot (y_T - y)^2 = 0.0393573728$\n",
        "\n",
        "Backprop:\n",
        "\n",
        "- $\\sigma'(z_y) = y(1 - y) = 0.1712852543$\n",
        "- $\\delta_y = (y - y_T)\\sigma'(z_y) = -0.04805604495$\n",
        "- $\\sigma'(z_h) = h(1 - h) = 0.1966119332$\n",
        "- $\\delta_h = (w_{hy} \\cdot \\delta_y)\\sigma'(z_h) = -0.00944839190$\n",
        "\n",
        "Partielle Ableitungen:\n",
        "\n",
        "- $\\frac{\\partial E}{\\partial w_{hy}} = \\delta_y \\cdot h = -0.03513178391$\n",
        "- $\\frac{\\partial E}{\\partial w_{xy}} = \\delta_y \\cdot x = 0$\n",
        "- $\\frac{\\partial E}{\\partial w_{by}} = \\delta_y = -0.04805604495$\n",
        "- $\\frac{\\partial E}{\\partial w_{xh}} = \\delta_h \\cdot x = 0$\n",
        "- $\\frac{\\partial E}{\\partial w_{bh}} = \\delta_h = -0.00944839190$\n",
        "\n",
        "Gewichtsupdates ($\\alpha = 0.01$):\n",
        "\n",
        "- $w_{hy} \\leftarrow 1 - 0.01 \\cdot \\frac{\\partial E}{\\partial w_{hy}} = 1.00035131784$\n",
        "- $w_{xy} \\leftarrow 2 - 0.01 \\cdot \\frac{\\partial E}{\\partial w_{xy}} = 2.0$\n",
        "- $w_{by} \\leftarrow -2 - 0.01 \\cdot \\frac{\\partial E}{\\partial w_{by}} = -1.99951943955$\n",
        "- $w_{xh} \\leftarrow -1 - 0.01 \\cdot \\frac{\\partial E}{\\partial w_{xh}} = -1.0$\n",
        "- $w_{bh} \\leftarrow 1 - 0.01 \\cdot \\frac{\\partial E}{\\partial w_{bh}} = 1.00009448392$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NN.Backprop.03\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f15cfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_iris(path):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with open(path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            features.append([float(v) for v in row[:4]])\n",
        "            labels.append(row[4])\n",
        "\n",
        "    classes = sorted(set(labels))\n",
        "    class_to_idx = {name: i for i, name in enumerate(classes)}\n",
        "    y = np.zeros((len(labels), len(classes)), dtype=np.float64)\n",
        "    for i, label in enumerate(labels):\n",
        "        y[i, class_to_idx[label]] = 1.0\n",
        "\n",
        "    x = np.array(features, dtype=np.float64)\n",
        "    mean = x.mean(axis=0)\n",
        "    std = x.std(axis=0)\n",
        "    std[std == 0] = 1.0\n",
        "    x = (x - mean) / std\n",
        "\n",
        "    return x, y, classes\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -60.0, 60.0)\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0.0, z)\n",
        "\n",
        "\n",
        "def relu_grad(z):\n",
        "    return (z > 0.0).astype(np.float64)\n",
        "\n",
        "\n",
        "def bce_loss(y_true, y_pred, eps=1e-9):\n",
        "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
        "    return -np.sum(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layer_sizes, seed=7):\n",
        "        if len(layer_sizes) < 2:\n",
        "            raise ValueError(\"Need at least input and output layer sizes\")\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.weights = []\n",
        "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            limit = math.sqrt(6.0 / (in_size + out_size))\n",
        "            w = rng.uniform(-limit, limit, size=(in_size + 1, out_size))\n",
        "            self.weights.append(w)\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = [x]\n",
        "        pre_acts = []\n",
        "        for idx, w in enumerate(self.weights):\n",
        "            a_prev = activations[-1]\n",
        "            a_prev_bias = np.concatenate(([1.0], a_prev))\n",
        "            z = a_prev_bias @ w\n",
        "            pre_acts.append(z)\n",
        "            if idx == len(self.weights) - 1:\n",
        "                a = sigmoid(z)\n",
        "            else:\n",
        "                a = relu(z)\n",
        "            activations.append(a)\n",
        "        return activations, pre_acts\n",
        "\n",
        "    def predict(self, x):\n",
        "        activations, _ = self.forward(x)\n",
        "        return activations[-1]\n",
        "\n",
        "    def train_epoch(self, x, y, lr):\n",
        "        indices = list(range(len(x)))\n",
        "        random.shuffle(indices)\n",
        "        for i in indices:\n",
        "            activations, pre_acts = self.forward(x[i])\n",
        "            delta = activations[-1] - y[i]\n",
        "            for layer_idx in range(len(self.weights) - 1, -1, -1):\n",
        "                a_prev = activations[layer_idx]\n",
        "                a_prev_bias = np.concatenate(([1.0], a_prev))\n",
        "                w_current = self.weights[layer_idx]\n",
        "                grad_w = np.outer(a_prev_bias, delta)\n",
        "                if layer_idx > 0:\n",
        "                    w_no_bias = w_current[1:, :]\n",
        "                    delta = w_no_bias @ delta\n",
        "                    delta *= relu_grad(pre_acts[layer_idx - 1])\n",
        "                self.weights[layer_idx] = w_current - lr * grad_w\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        total = 0.0\n",
        "        for i in range(len(x)):\n",
        "            y_pred = self.predict(x[i])\n",
        "            total += bce_loss(y[i], y_pred)\n",
        "        return total / len(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(x, y, layer_sizes, lr=0.01, epochs=3000, seed=7):\n",
        "    model = MLP(layer_sizes, seed=seed)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train_epoch(x, y, lr)\n",
        "        losses.append(model.loss(x, y))\n",
        "    return model, losses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['setosa', 'versicolor', 'virginica']\n",
            "Final loss: 0.041727\n",
            "Epoch with loss < 0.05: 5335\n"
          ]
        }
      ],
      "source": [
        "data_path = os.path.join(\"iris.csv\")\n",
        "x, y, classes = load_iris(data_path)\n",
        "\n",
        "layer_sizes = [x.shape[1], 16, 8, len(classes)]\n",
        "model, losses = train_model(x, y, layer_sizes, lr=0.0005, epochs=6000, seed=7)\n",
        "\n",
        "near_zero_epoch = None\n",
        "for i, loss in enumerate(losses, start=1):\n",
        "    if loss < 0.05:\n",
        "        near_zero_epoch = i\n",
        "        break\n",
        "print(\"Classes:\", classes)\n",
        "print(\"Final loss:\", f\"{losses[-1]:.6f}\")\n",
        "if near_zero_epoch is not None:\n",
        "    print(\"Epoch with loss < 0.05:\", near_zero_epoch)\n",
        "else:\n",
        "    print(\"Loss did not fall below 0.05 within the given epochs\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
